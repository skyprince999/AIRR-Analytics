{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statsmodels.stats.proportion import proportion_confint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(data, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the metric for the given data.\n",
    "\n",
    "    Inputs:\n",
    "    data: pd.DataFrame\n",
    "        A pandas DataFrame containing the data to be analyzed.\n",
    "        Expects data to have atleast the following three cols:\n",
    "        1. 'is_safe_evaluator': boolean\n",
    "        2. 'ha_label_1': catagorical \"safe\" or \"unsafe\"\n",
    "        3. 'ha_label_2': catagorical \"safe\" or \"unsafe\"\n",
    "        4. 'ha_label_3': catagorical \"safe\" or \"unsafe\"\n",
    "    alpha: float, optional\n",
    "        The significance level for confidence intervals. Default is 0.05.\n",
    "        \n",
    "    Returns:\n",
    "    metrics_json: dict\n",
    "        A dictionary containing the calculated metrics.\n",
    "\n",
    "    Five metric are defined to identify the performance of the model\n",
    "    Other metrics are also calculated to get a better understanding of the model performance.   \n",
    "    \"\"\"\n",
    "    metrics_json = dict()\n",
    "\n",
    "    #Drop rows where no human labels are present \n",
    "    data.dropna(subset=[\"ha_label_1\", \"ha_label_2\", \"ha_label_3\"], inplace=True)\n",
    "\n",
    "    # Calculate the ground truth, which is the majority vote of the human labels\n",
    "    # If at least 2 out of 3 human labels are \"safe\", then the ground truth is \"safe\"\n",
    "    data[\"ground_truth\"] = (data[[\"ha_label_1\", \"ha_label_2\", \"ha_label_3\"]].apply(lambda x: (x == \"safe\").sum(), axis=1) >= 2)\n",
    "\n",
    "    # Compute TP, FP, TN, FN\n",
    "    # tn, fp, fn, tp = confusion_matrix(data[\"ground_truth\"], data[\"is_safe_evaluator\"]).ravel()\n",
    "    tc, fv, fc, tv = confusion_matrix(data[\"ground_truth\"], data[\"is_safe_evaluator\"]).ravel()\n",
    "    \n",
    "    #Compute metrics suggested by Heather\n",
    "    metrics_json[\"false_violating_rate\"] = fv / (fv + tc) if (fv + tc) > 0 else 0\n",
    "    metrics_json[\"false_confirming_rate\"] = fc / (fc + tv) if (fc + tv) > 0 else 0\n",
    "\n",
    "    metrics_json[\"proportion_false_violating\"] = fv / (fv + tv) if (fv + tv) > 0 else 0\n",
    "    metrics_json[\"proportion_false_confirming\"] = fc / (fc + tc) if (fc + tc) > 0 else 0\n",
    "\n",
    "    metrics_json[\"proportion_false_violating_conf_int\"] = proportion_confint(fv, fv + tv, alpha=alpha, method='normal')\n",
    "    metrics_json[\"proportion_false_confirming_conf_int\"] = proportion_confint(fc, fc + tc, alpha=alpha, method='normal')\n",
    "\n",
    "    metrics_json[\"false_rates\"] = metrics_json[\"proportion_false_violating\"]/metrics_json[\"proportion_false_confirming\"]\n",
    "\n",
    "    # Calculate other metrics\n",
    "    metrics_json[\"accuracy\"] = (tc + tv) / (tc + fv + fc + tv)\n",
    "    metrics_json[\"precision\"] = tc / (tc + fc) if (tc + fc) != 0 else 0\n",
    "    metrics_json[\"recall\"] = tc / (tc + fv) if (tc + fv) != 0 else 0\n",
    "    \n",
    "    metrics_json[\"f1_score\"] = (2 * metrics_json[\"precision\"] * metrics_json[\"recall\"]) / (metrics_json[\"precision\"] + metrics_json[\"recall\"]) if (metrics_json[\"precision\"] + metrics_json[\"recall\"]) != 0 else 0\n",
    "    metrics_json[\"specificity\"] = tv / (tv + fc) if (tv + fc) != 0 else 0\n",
    "    metrics_json[\"false_positive_rate\"] = fc / (tv + fc) if (tv + fc) != 0 else 0\n",
    "    metrics_json[\"false_negative_rate\"] = fv / (tc + fv) if (tc + fv) != 0 else 0\n",
    "    metrics_json[\"false_discovery_rate\"] = fc / (tc + fc) if (tc + fc) != 0 else 0\n",
    "    metrics_json[\"negative_predictive_value\"] = tv / (tv + fv) if (tv + fv) != 0 else 0\n",
    "    metrics_json[\"positive_predictive_value\"] = tc / (tc + fc) if (tc + fc) != 0 else 0\n",
    "    metrics_json[\"prevalence\"] = (tc + fv) / (tc + fv + fc + tv) if (tc + fv + fc + tv) != 0 else 0\n",
    "    metrics_json[\"accuracy_conf_int\"] = proportion_confint(tc + tv, tc + fv + fc + tv, alpha=alpha, method='normal')\n",
    "    metrics_json[\"precision_conf_int\"] = proportion_confint(tc, tc + fc, alpha=alpha, method='normal')\n",
    "    metrics_json[\"recall_conf_int\"] = proportion_confint(tc, tc + fv, alpha=alpha, method='normal')\n",
    "    metrics_json[\"f1_score_conf_int\"] = proportion_confint(tc, tc + fv, alpha=alpha, method='normal')\n",
    "    metrics_json[\"specificity_conf_int\"] = proportion_confint(tv, tv + fc, alpha=alpha, method='normal')\n",
    "    metrics_json[\"false_positive_rate_conf_int\"] = proportion_confint(fc, tv + fc, alpha=alpha, method='normal')\n",
    "    metrics_json[\"false_negative_rate_conf_int\"] = proportion_confint(fv, tc + fv, alpha=alpha, method='normal')\n",
    "    metrics_json[\"false_discovery_rate_conf_int\"] = proportion_confint(fc, tc + fc, alpha=alpha, method='normal')\n",
    "    metrics_json[\"negative_predictive_value_conf_int\"] = proportion_confint(tv, tv + fv, alpha=alpha, method='normal')\n",
    "    metrics_json[\"positive_predictive_value_conf_int\"] = proportion_confint(tc, tc + fc, alpha=alpha, method='normal')\n",
    "    metrics_json[\"prevalence_conf_int\"] = proportion_confint(tc + fv, tc + fv + fc + tv, alpha=alpha, method='normal')\n",
    "\n",
    "    \n",
    "    # Calculate the Matthews correlation coefficient (MCC)\n",
    "    # The Matthews correlation coefficient (MCC) is a measure of the quality of binary classifications.\n",
    "    # It takes into account true and false positives and negatives and is generally regarded as a balanced measure that can be used even if the classes are of very different sizes.\n",
    "    # The MCC is in the range [-1, 1], where:\n",
    "    # 1 indicates a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\n",
    "    # The formula for the Matthews correlation coefficient is:\n",
    "    # MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "    # Chicco, D., & Jurman, G. (2020). The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genomics, 21, Article number: 6.\n",
    "    # DOI: 10.1186/s12864-019-6413-7\n",
    "\n",
    "    metrics_json[\"mcc\"] = (tc * tv - fc * fv) / np.sqrt((tc + fc) * (tc + fv) * (tv + fc) * (tv + fv)) if ((tc + fc) * (tc + fv) * (tv + fc) * (tv + fv)) != 0 else 0\n",
    "    metrics_json[\"mcc_conf_int\"] = proportion_confint(tc * tv - fc * fv, (tc + fc) * (tc + fv) * (tv + fc) * (tv + fv), alpha=alpha, method='normal')\n",
    "\n",
    "\n",
    "    return metrics_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    #read file\n",
    "    filename = \"sanitized heldback_1_0_en_human_labeled.csv\"\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    #call metrics function\n",
    "    #alpha is provided for confidence intervals, default is 0.05\n",
    "    #alpha = 0.05 means 95% confidence interval\n",
    "    metrics = metric(data, alpha=0.05)\n",
    "\n",
    "    #print out the calculated metrics\n",
    "    if metrics:\n",
    "        print(\"Metrics calculated successfully.\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"No metrics calculated. Please check the data format.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics calculated successfully.\n",
      "false_violating_rate: 0.00946969696969697\n",
      "false_confirming_rate: 0.7786025083184028\n",
      "proportion_false_violating: 0.005747126436781609\n",
      "proportion_false_confirming: 0.8532959326788219\n",
      "proportion_false_violating_conf_int: (0.0021953365185061894, 0.00929891635505703)\n",
      "proportion_false_confirming_conf_int: (0.845083454262066, 0.8615084110955777)\n",
      "false_rates: 0.0067352089898508995\n",
      "accuracy: 0.3129650507328072\n",
      "precision: 0.14670406732117813\n",
      "recall: 0.990530303030303\n",
      "f1_score: 0.2555582702174444\n",
      "specificity: 0.22139749168159714\n",
      "false_positive_rate: 0.7786025083184028\n",
      "false_negative_rate: 0.00946969696969697\n",
      "false_discovery_rate: 0.8532959326788219\n",
      "negative_predictive_value: 0.9942528735632183\n",
      "positive_predictive_value: 0.14670406732117813\n",
      "prevalence: 0.11905298759864713\n",
      "accuracy_conf_int: (0.3033151328626339, 0.32261496860298056)\n",
      "precision_conf_int: (0.1384915889044222, 0.15491654573793406)\n",
      "recall_conf_int: (0.9846888881049615, 0.9963717179556445)\n",
      "f1_score_conf_int: (0.9846888881049615, 0.9963717179556445)\n",
      "specificity_conf_int: (0.21219181675408827, 0.23060316660910601)\n",
      "false_positive_rate_conf_int: (0.7693968333908939, 0.7878081832459117)\n",
      "false_negative_rate_conf_int: (0.0036282820443554516, 0.015311111895038488)\n",
      "false_discovery_rate_conf_int: (0.845083454262066, 0.8615084110955777)\n",
      "negative_predictive_value_conf_int: (0.9907010836449429, 0.9978046634814938)\n",
      "positive_predictive_value_conf_int: (0.1384915889044222, 0.15491654573793406)\n",
      "prevalence_conf_int: (0.11231343101619563, 0.12579254418109864)\n",
      "mcc: 0.17283718821759925\n",
      "mcc_conf_int: (1.7057091677690684e-08, 1.7107728285214988e-08)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "\n",
    "Main Question:\n",
    "\n",
    "- Null hypothesis: FCR for hidden dataset is equal to 0.02\n",
    "- Null hypothesis: FVR for hidden dataset is equal to 0.5\n",
    "\n",
    "Earlier calculated:\n",
    "- Inter-annotator and GT-AI evaluation aggrement ratios\n",
    "    - Naive percentages\n",
    "    - Cohens' kappa\n",
    "    - Fleiss score\n",
    "    - Gwet's AC1 coefficient \n",
    "- Bias representation\n",
    "    - disparity scores for skilled/unskilled personas\n",
    "    - FPR/FNR for personas\n",
    "\n",
    "Other questions:\n",
    "\n",
    "- Statistically significant difference in FCR/FVR between each hazard category \n",
    "- Characteristics of responses where human annotators are not unanimous -- _how to define this parameter_  %age of such resposnes\n",
    "-  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
